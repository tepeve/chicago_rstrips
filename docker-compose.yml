
x-airflow-common:
  &airflow-common
  build: .
  environment:
    # Configuración del ejecutor de Airflow
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    # Conexión a la METADATA DATABASE de Airflow
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow_db:5432/airflow_db
    # Acá le indicamos a Airflow que tu paquete `chicago_rstrips` está en /opt/airflow/src
    - PYTHONPATH=/opt/airflow/src
    # Clave secreta para el webserver de Airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
    # UID/GID del usuario host para asignar ownership a los bind-mounts dentro del contenedor
    - HOST_UID=${HOST_UID}
    - HOST_GID=${HOST_GID}
  volumes:
    # Monta tus carpetas locales en el contenedor.
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src 
    - ./data:/opt/airflow/data 
    - ./.env:/opt/airflow/.env      
    - ./sql:/opt/airflow/sql  
    - ./tests:/opt/airflow/tests 
    - ./outputs:/opt/airflow/outputs 

  depends_on:
    - postgres_airflow_db

services:
  # ------------------------------------------------------------------
  # SERVICIO 1: BASE DE DATOS LOCAL - DATA WAREHOUSE
  # ------------------------------------------------------------------
  postgres_local_db:
    image: postgres:14
    environment:
      - POSTGRES_USER=${POSTGRES_LOCAL_USER}      # <-- defino las credenciales como iguales a las de Redshift
      - POSTGRES_PASSWORD=${POSTGRES_LOCAL_PASSWORD} # <--  defino las credenciales como iguales a las de Redshift
      - POSTGRES_DB=${POSTGRES_LOCAL_DB}     # <--  defino las credenciales como iguales a las de Redshift
    ports:
      # Expone el puerto 5432 del contenedor en el puerto 5433 de tu PC
      - "5433:5432"
    volumes:
      # Persiste los datos de tu warehouse en un volumen de Docker
      - postgres_data:/var/lib/postgresql/data

  # ------------------------------------------------------------------
  # SERVICIO 2: METADATA DATABASE DE AIRFLOW
  # ------------------------------------------------------------------
  postgres_airflow_db:
    image: postgres:14
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow_db
    volumes:
      - airflow_db_data:/var/lib/postgresql/data

  # ------------------------------------------------------------------
  # SERVICIO 3: INICIALIZADOR DE AIRFLOW
  # (Se ejecuta una vez para preparar la BD de metadatos)
  # ------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    user: "0:0"  # Forzar ejecución como root (UID 0, GID 0)
    entrypoint: /opt/airflow/entrypoint.sh
    command:
      - bash
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
    restart: "no"

  # ------------------------------------------------------------------
  # SERVICIO 4: WEBSERVER DE AIRFLOW
  # ------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    command: airflow webserver
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      postgres_airflow_db:
        condition: service_started # Espera a que la BD esté lista
      airflow-init:
        condition: service_completed_successfully

  # ------------------------------------------------------------------
  # SERVICIO 5: SCHEDULER DE AIRFLOW 
  # ------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    restart: always
    depends_on:
      postgres_airflow_db:
        condition: service_started
      airflow-init:
        condition: service_completed_successfully

# Define los volúmenes para persistir los datos
volumes:
  postgres_data:
  airflow_db_data: